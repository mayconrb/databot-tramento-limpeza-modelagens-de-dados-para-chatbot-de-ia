# -*- coding: utf-8 -*-
"""DataBot_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OBPvQdpAqtfAbdHM9Hr9asXAXtAEoeNl
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
!pip install spacy
!pip install nltk
!pip install tensorflow

!pip install --upgrade tensorflow
!pip install --upgrade keras

import nltk
import numpy as np
import pandas as pd
import seaborn as sns

from nltk.tokenize import sent_tokenize
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
import string

from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
from nltk import pos_tag
from wordcloud import WordCloud

from nltk import ngrams
from collections import Counter

from nltk.corpus import stopwords
nltk.download('stopwords')

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk
from nltk import ne_chunk

import re
import matplotlib.pyplot as plt

import spacy
nlp = spacy.load('en_core_web_sm')

from transformers import BertModel, BertTokenizer
from sklearn.model_selection import train_test_split
import copy

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from sklearn.model_selection import train_test_split


from IPython.display import clear_output
import torch.optim as optim
from torch.utils.data import random_split, Dataset, DataLoader

import torch.nn as nn
import torch
from tqdm import tqdm

df1 = pd.read_csv("/content/drive/MyDrive/Estudos/FACENS/A2023S1/Processamento de Linguagem Natural/AF_Chatbot/S08_question_answer_pairs.txt", delimiter='\t', encoding='latin1')
df2 = pd.read_csv("/content/drive/MyDrive/Estudos/FACENS/A2023S1/Processamento de Linguagem Natural/AF_Chatbot/S09_question_answer_pairs.txt", delimiter='\t', encoding='latin1')
df3 = pd.read_csv("/content/drive/MyDrive/Estudos/FACENS/A2023S1/Processamento de Linguagem Natural/AF_Chatbot/S10_question_answer_pairs.txt", delimiter='\t', encoding='latin1')

df1 = df1.rename(columns={'ArticleTitle': 'titulo artigo', 'Question': 'pergunta', 'Answer': 'resposta'})
df2 = df2.rename(columns={'ArticleTitle': 'titulo artigo', 'Question': 'pergunta', 'Answer': 'resposta'})
df3 = df3.rename(columns={'ArticleTitle': 'titulo artigo', 'Question': 'pergunta', 'Answer': 'resposta'})

df_concat = pd.concat([df1, df2, df3], ignore_index=True)
display(df_concat)

df = df_concat[['titulo artigo', 'pergunta', 'resposta']]
display(df)
df.shape

display(df)

df = df[['titulo artigo', 'pergunta', 'resposta']]
df = df.apply(lambda x: x.str.lower().str.strip())

display(df)

stopwords_english = set(stopwords.words('english'))
df['titulo artigo'] = df['titulo artigo'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stopwords_english]))
df['titulo artigo'] = df['titulo artigo'].apply(lambda x: re.sub(r'\b\w\b', ' ', x))
df['pergunta'] = df['pergunta'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stopwords_english]))
df['pergunta'] = df['pergunta'].apply(lambda x: re.sub(r'\b\w\b', ' ', x))


display(df)

# def remove_special_characters(text):
#     text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
#     return text

def remove_special_characters(text):
    text = re.sub(r"""[`,|'*?¨(-)!@#[$%/\]~^´.+]+""", '', text)
    return text

re.sub(r"""[`,|'*?¨(-)!@#[$%/\]~^´.+]+""", '', '5 or 6')

df['titulo artigo'] = df['titulo artigo'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stopwords_english]))
df['titulo artigo'] = df['titulo artigo'].apply(remove_special_characters)

df['resposta'] = df['resposta'].apply(lambda x: ' '.join([word.strip() for word in str(x).strip().split(' ')]))
df['resposta'] = df['resposta'].apply(remove_special_characters)

df['pergunta'] = df['pergunta'].apply(lambda x: ' '.join([word.strip() for word in str(x).strip().split(' ') if word.strip() not in stopwords_english]))
df['pergunta'] = df['pergunta'].apply(remove_special_characters)

display(df)

df = df[(df['resposta'] != 'nan') & (df['resposta'] != '')]

df

# rows_null = df[df.isna().any(axis=1)]
# print("Linhas Vazias: ", rows_null)

# df.shape

# rows_null = df[df.isnull().any(axis=1)]
# print("Linhas Vazias: ", rows_null)

# df = df.dropna()

text_pergunta = ' '.join(df['pergunta'])
print('NUVEM PALAVRAS PERGUNTA')

wordcloud = WordCloud(width=800, height=400, max_font_size=150, background_color='white').generate(text_pergunta)

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

text_resposta = ' '.join(df['resposta'])
print('NUVEM PALAVRAS RESPOSTAS"')

wordcloud = WordCloud(width=800, height=400, max_font_size=150, background_color='white').generate(text_resposta)

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Tokenizar as palavras
tokens = nltk.word_tokenize(text_pergunta)

# Calcular a frequência das palavras
freq_dist = nltk.FreqDist(tokens)

# Obter as palavras e suas frequências
words = freq_dist.keys()
frequencies = freq_dist.values()

# Ordenar as palavras por frequência (do maior para o menor)
sorted_words = sorted(words, key=lambda w: freq_dist[w], reverse=True)
sorted_frequencies = [freq_dist[w] for w in sorted_words]

# Selecionar as 30 palavras mais frequentes
top_words = sorted_words[:50]
top_frequencies = sorted_frequencies[:50]

# Criar o gráfico de barras das palavras mais frequentes
plt.figure(figsize=(12, 6))
plt.bar(range(len(top_words)), top_frequencies)
plt.xticks(range(len(top_words)), top_words, rotation='vertical')
plt.xlabel('Palavras')
plt.ylabel('Frequência')
plt.title('30 Palavras Mais Frequentes PERGUNTAS')
plt.tight_layout()
plt.show()

df_copia = copy.copy(df)


# Converter os valores da coluna 'resposta' para strings
df_copia['resposta'] = df_copia['resposta'].astype(str)

# Obter todas as palavras da coluna 'resposta'
all_words = ' '.join(df_copia['resposta'])

# Criar uma lista de stopwords em inglês
stopwords_english = set(stopwords.words('english'))

# Função para remover stopwords da lista de palavras
def remove_stopwords(text):
    words = text.split()
    words = [word for word in words if word.lower() not in stopwords_english]
    return ' '.join(words)

# Remover stopwords da lista de palavras
all_words = remove_stopwords(all_words)

# Calcular a frequência das palavras
word_freq = {}
for word in all_words.split():
    word_freq[word] = word_freq.get(word, 0) + 1

# Ordenar as palavras por frequência (do maior para o menor)
sorted_words = sorted(word_freq.keys(), key=lambda w: word_freq[w], reverse=True)
sorted_frequencies = [word_freq[w] for w in sorted_words]

# Selecionar as 30 palavras mais frequentes
top_words = sorted_words[:30]
top_frequencies = sorted_frequencies[:30]

# Criar o gráfico de barras das palavras mais frequentes
plt.figure(figsize=(12, 6))
plt.bar(range(len(top_words)), top_frequencies)
plt.xticks(range(len(top_words)), top_words, rotation='vertical')
plt.xlabel('Palavras')
plt.ylabel('Frequência')
plt.title('30 Palavras Mais Frequentes RESPOSTA')
plt.tight_layout()
plt.show()

# Extrair as perguntas do dataset
perguntas = df['pergunta']

# Tokenizar as palavras das perguntas e converter para strings
tokens = [word_tokenize(pergunta.lower()) for pergunta in perguntas]
tokens = [' '.join(token) for token in tokens]

# Definir o valor de n para os n-grams
n = 2

# Gerar os n-grams das perguntas
ngrams_list = [ngrams(token.split(), n) for token in tokens]
ngrams_flat = [gram for sublist in ngrams_list for gram in sublist]

# Contar a frequência dos n-grams
ngrams_counts = Counter(ngrams_flat)

# Selecionar os 15 n-grams mais comuns
top_ngrams = ngrams_counts.most_common(50)

# Extrair as palavras e contagens para o gráfico de barras
ngrams_words = [gram[0] for gram in top_ngrams]
ngrams_counts = [gram[1] for gram in top_ngrams]

print('-------------------')
print('N-GRAMS 2 Palavras')
print('-------------------')
top_ngrams

# Definir o valor de n para os n-grams
n = 3

# Gerar os n-grams das perguntas
ngrams_list = [ngrams(token.split(), n) for token in tokens]
ngrams_flat = [gram for sublist in ngrams_list for gram in sublist]

# Contar a frequência dos n-grams
ngrams_counts = Counter(ngrams_flat)

# Selecionar os 15 n-grams mais comuns
top_ngrams = ngrams_counts.most_common(50)

# Extrair as palavras e contagens para o gráfico de barras
ngrams_words = [gram[0] for gram in top_ngrams]
ngrams_counts = [gram[1] for gram in top_ngrams]

print('-------------------')
print('N-GRAMS 2 Palavras')
print('-------------------')
top_ngrams

# Carregar o modelo do spaCy para análise sintática
nlp = spacy.load('en_core_web_sm')

# Função para realizar a análise sintática de uma frase
def analyze_syntax(text):
    doc = nlp(text)
    syntax_analysis = []
    for token in doc:
        syntax_analysis.append({
            'Palavra': token.text,
            'Tag': token.tag_,
            'Dependência': token.dep_,
            'Lemma': token.lemma_,
            'Entidade': token.ent_type_
        })
    return syntax_analysis

# Realizar a análise sintática para cada pergunta no DataFrame
df['Análise Sintática Pergunta'] = df['pergunta'].apply(analyze_syntax)

# Realizar a análise sintática para cada pergunta no DataFrame
df['Análise Sintática Resposta'] = df['resposta'].apply(analyze_syntax)

# Realizar a análise sintática para cada pergunta no DataFrame
df['Análise Sintática Titulo Artigo'] = df['titulo artigo'].apply(analyze_syntax)

# Exibir o DataFrame com a análise sintática
display(df)

df.shape

# Função para realizar a análise semântica em uma frase
def perform_semantic_analysis(text):
    doc = nlp(text)
    # Acessar as informações semânticas no objeto Doc
    # Exemplo: doc.ents para entidades nomeadas, doc.sents para sentenças, etc.
    # Você pode extrair as informações relevantes para o seu caso de uso

    # Exemplo de impressão das entidades nomeadas
    for entity in doc.ents:
        print(entity.text, entity.label_)

# PERSON: nomes próprios de pessoas
# GPE (Geo-Political Entity) entidades geopolíticas, como países, cidades, estados
# ORG (Organization): organizações, como empresas, instituições
# CARDINAL: números cardinais
# DATE: DATA E orientação de Dias como ontem, amanhã
# NORP (Nationalities or Religious or Political Groups):  nacionalidades, grupos religiosos ou políticos, como "americano", "cristão", "democrata", etc.
# LANGUAGE: idiomas
# LOC (Location): localização como "praia", "montanha",

# Aplicar a análise semântica nas colunas relevantes do seu dataset
df['pergunta'].apply(perform_semantic_analysis)
df['resposta'].apply(perform_semantic_analysis)
df['titulo artigo'].apply(perform_semantic_analysis)

display(df)

# Função para extrair entidades de um texto
def extract_entities(text):
    entities = []
    sentences = nltk.sent_tokenize(text)
    for sentence in sentences:
        words = nltk.word_tokenize(sentence)
        tagged_words = nltk.pos_tag(words)
        chunked_words = ne_chunk(tagged_words)
        for tree in chunked_words:
            if hasattr(tree, 'label'):
                entity = ' '.join(c[0] for c in tree.leaves())
                entities.append((entity, tree.label()))
    return entities

# Aplicar a função em uma coluna do DataFrame
df['entidades'] = df['pergunta'].apply(extract_entities)

# Função para extrair entidades
def extract_entities(text):
    entities = []
    sentences = nltk.sent_tokenize(text)
    for sentence in sentences:
        words = nltk.word_tokenize(sentence)
        tagged_words = nltk.pos_tag(words)
        chunked_words = nltk.ne_chunk(tagged_words)
        entities.extend(extract_entity_names(chunked_words))
    return entities

# Função auxiliar para extrair nomes de entidades
def extract_entity_names(t):
    entity_names = []
    if hasattr(t, 'label') and t.label:
        if t.label() in ['PERSON', 'ORGANIZATION', 'GPE']:
            entity_names.append(' '.join([child[0] for child in t]))
        else:
            for child in t:
                entity_names.extend(extract_entity_names(child))
    return entity_names

# Aplicar análise semântica em cada linha do dataset
for index, row in df.iterrows():
    # Extrair o texto da pergunta
    pergunta = row["pergunta"]

    # Aplicar análise semântica
    doc = nlp(pergunta)

    # Acessar as entidades nomeadas
    for entidade in doc.ents:
        print(entidade.text, entidade.label_)

maior_pergunta = 0
for i in range(df.shape[0]):
  try:
    linha = len(str(df['pergunta'][i]).strip())
    if linha > maior_pergunta:
      maior_pergunta = linha
  except:
    continue

print(maior_pergunta)

maior_resposta = 0
maior_i = 0
r = 0
for i in range(df.shape[0]):
  try:
    linha = len(str(df['resposta'][i]).strip())
    if linha > maior_resposta:
      maior_i = i
      maior_resposta = linha
      r = str(df['resposta'][i]).strip()
  except:
    continue

print(maior_resposta)
print(i)

dict_palavra = {'<star>':0, '<stop>':1, '<pad>':2}

for pergunta in df['pergunta']:
  pergunta = str(pergunta).split(' ')
  for palavra in pergunta:
    palavra = palavra.strip()
    if palavra not in dict_palavra:
      dict_palavra[palavra] = len(dict_palavra)

for resposta in df['resposta']:
  resposta = str(resposta).split(' ')
  for palavra in resposta:
    palavra = palavra.strip()
    if palavra not in dict_palavra:
      dict_palavra[palavra] = len(dict_palavra)

len(dict_palavra)

df.head()

class Dataset_texto(Dataset):

    def __init__(self, pergunta, resposta) -> None:

        self.pergunta = pergunta
        self.resposta = resposta
        self.x, self.y = self.gerar_dataset()


    def __len__(self) -> int:
        return len(self.x)


    def __getitem__(self, index: int) -> torch.Tensor:

        x = self.x[index]
        y = self.y[index]

        x = torch.from_numpy(x)
        y = torch.from_numpy(y)

        return x, y


    def gerar_dataset(self) -> np.ndarray:

        df_pergunta = []

        for pergunta in self.pergunta:
          pergunta = str(pergunta).strip().split(' ')
          novo_texto = [dict_palavra['<star>']]
          for palavra in pergunta:
            novo_texto.append(dict_palavra[palavra])

          novo_texto.append(dict_palavra['<stop>'])

          while len(novo_texto) < maior_pergunta:
            novo_texto.append(dict_palavra['<pad>'])

          df_pergunta.append(novo_texto)

        df_pergunta = np.array(df_pergunta)

        df_resposta = []



        for resposta in self.resposta:
          resposta = str(resposta).strip().split(' ')
          novo_texto = [dict_palavra['<star>']]
          for palavra in resposta:
            novo_texto.append(dict_palavra[palavra])

          novo_texto.append(dict_palavra['<stop>'])

          while len(novo_texto) < maior_resposta:
            novo_texto.append(dict_palavra['<pad>'])

          df_resposta.append(novo_texto)

        df_resposta = np.array(df_resposta)


        return df_pergunta, df_resposta

dataset = df[['pergunta', 'resposta']]
dataset

X_train, X_test, y_train, y_test = train_test_split(dataset['pergunta'], dataset['resposta'], test_size=0.3, random_state=42)

X_train

y_train

treino = pd.concat([X_train, y_train], axis = 1)
teste = pd.concat([X_test, y_test], axis = 1)

treino

teste

class_dataset_treino = Dataset_texto(treino['pergunta'], treino['resposta'])
class_dataset_teste = Dataset_texto(teste['pergunta'], teste['resposta'])

for x, y in class_dataset_teste:
  break
print(x,y)

class Modelo_LSTM(nn.Module):

    def __init__(self, out_shape, dic_palavra, device) -> None:
        super(Modelo_LSTM, self).__init__()

        self.out_shape_x, self.out_shape_y = out_shape
        self.out_shape_y -= 1
        self.dic_palavra = dic_palavra
        self.device = device

        self.q_emb = nn.Embedding(num_embeddings=self.out_shape_x, embedding_dim=self.out_shape_y)
        self.q_lstm = nn.LSTM(input_size=self.out_shape_y, hidden_size=256, bias=True, batch_first=True, num_layers=1)
        self.q_lin = nn.Linear(in_features=256, out_features=128)

        self.a_emb = nn.Embedding(num_embeddings=self.out_shape_x, embedding_dim=self.out_shape_y)
        self.a_lstm = nn.LSTM(input_size=self.out_shape_y + 128, hidden_size=256, bias=True, batch_first=False, num_layers=1)

        self.lin = nn.Linear(in_features=256, out_features=self.out_shape_x)
        self.drop = nn.Dropout(p=0.4)

        self.to(device=device)

    def forward(self, question: torch.Tensor, answer: torch.Tensor) -> torch.Tensor:

        pred = torch.zeros(size=[question.size(0), self.out_shape_x, self.out_shape_y]).to(device=self.device)

        question = self.q_emb(question.long())
        q_out, (h, c) = self.q_lstm(question)
        q_out = self.q_lin(h)

        answer = self.a_emb(answer.long())

        for idx in range(self.out_shape_y):

            input_lstm = torch.cat(tensors=[answer[:,idx].unsqueeze(0), q_out], dim=2)
            out, (h, c) = self.a_lstm(input_lstm, (h, c))
            out_lin = self.lin(self.drop(out.squeeze(1)))

            pred[:,:,idx] = out_lin

        return pred

    def gen_text(self, question: torch.Tensor) -> torch.Tensor:

        pred = torch.zeros(size=(question.size(0), self.out_shape_x, self.out_shape_y)).to(device=self.device)
        init_word = torch.tensor(data=[self.wd['<start>']]).to(device=self.device)

        question = self.q_emb(question.long())
        q_out, (h, c) = self.q_lstm(question)
        q_out = self.q_lin(h)

        words = self.a_emb(init_word.long())

        for word_idx in range(self.out_shape_y):

            input_lstm = torch.cat(tensors=[words[:,0].unsqueeze(0), q_out], dim=2)
            out, (h,c) = self.a_lstm(input_lstm, (h,c))
            out_lin = self.lin(self.drop(out.squeeze(1)))

            pred[:,:,word_idx] = out_lin

            words = self.a_emb(out_lin)

            if out_lin.argmax() == self.wd['<stop>']:
                break

        return pred

device = "cuda" if torch.cuda.is_available() else "cpu"

model = Modelo_LSTM(out_shape=[len(dict_palavra), maior_resposta], dic_palavra=dict_palavra, device=device)

build = model(
    torch.zeros(size=(2,maior_pergunta)).to(device=device),
    torch.zeros(size=(2,maior_resposta)).to(device=device)
)
build.size()

epochs = 100
batch_size = 32
lr = 1e-3
early_stop = 5
n_plots = 10

def _init_weights(module) -> None:
    if isinstance(module, (nn.Linear, nn.Embedding)):
        module.weight.data.normal_(mean=0, std=0.02)

model.apply(_init_weights)

loss_fn = nn.CrossEntropyLoss(ignore_index=dict_palavra['<pad>'])

optimizer = optim.Adam(model.parameters(), lr=lr)

train_loader = DataLoader(dataset=class_dataset_treino, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(dataset=class_dataset_teste, batch_size=batch_size, shuffle=True)

best_loss = float('inf')
best_epoch = 'No saved yet'

previous_train = float('inf')
previous_valid = float('inf')

overfitting = 0


for curr_epoch in range(1, epochs+1):

    print(f'Epoch: {curr_epoch}/{epochs} | Batch: {batch_size} | Learing Rate: {lr} | Loss Function: {loss_fn.__class__.__name__} | Optimizer: {optimizer.__class__.__name__}')
    print(f'Overtting: {overfitting}/{early_stop} | Best loss: {best_loss} - Last savement: {best_epoch}')

    if overfitting == early_stop:
        print('\033[1;31mOVERFITTING\033[0m')
        break

    # treinamento
    train_iter = tqdm(iterable=train_loader)
    train_loss = 0
    model.train()
    for x, y in train_iter:

        x = x.to(device=device)
        y = y.to(device=device)

        yhat = model(x, y.long())

        loss = loss_fn(yhat.view(-1, len(dict_palavra)), y[:,1:].reshape(-1).long())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # resultado de treinamento
        train_iter.set_description(desc=f'Loss: {loss.item()}')
        train_loss += loss.item()

        del x, y, yhat, loss

    train_loss /= len(train_loader)

    # validacao
    valid_iter = tqdm(iterable=valid_loader)
    valid_loss = 0
    model.eval()
    with torch.no_grad():
        for x, y in valid_iter:

            x = x.to(device=device)
            y = y.to(device=device)

            yhat = model(x, y.long())

            loss = loss_fn(yhat.view(-1, len(dict_palavra)), y[:,1:].reshape(-1).long())

            # saving valid results
            valid_iter.set_description(desc=f'Valid loss: {loss.item()}')
            valid_loss += loss.item()

            del x, y, yhat, loss

    valid_loss /= len(valid_loader)

    # loss
    if best_loss > valid_loss:
        best_loss = valid_loss
        best_epoch = curr_epoch
        torch.save(obj=model.state_dict(), f='/content/drive/MyDrive/Colab Notebooks/modeloLSTN.pt')

    # overfits
    if previous_train > train_loss and previous_valid < valid_loss:
        overfitting += 1
    else:
        if overfitting > 0:
            overfitting -= 1
    previous_train = train_loss
    previous_valid = valid_loss

    clear_output()

